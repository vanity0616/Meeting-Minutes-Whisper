{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanity0616/Meeting-Minutes-Whisper/blob/main/Meeting_Minutes_Transcription_Using_OpenAI's_Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Meeting Minutes Transcription Using OpenAI's Whisper**\n",
        "\n",
        "Whisper (https://github.com/openai/whisper) is a general-purpose speech recognition model, which can perform multilingual speech recognition as well as speech translation. This Notebook will guide you how to record face-to-face meetings or in-person classes using Whisper.\n",
        "\n",
        "<font size='5'>[IMPORTANT]:\n",
        "\n",
        "1.   Be sure to run the following processes one by one, do not skip any steps.\n",
        "\n",
        "2.   Make sure you select GPU as hardware accelerator in notebook settings, otherwise the processing speed will be very slow."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Requirement**"
      ],
      "metadata": {
        "id": "8zRG_pLh4rx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHmCEB8LJvgc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install sounddevice wavio\n",
        "! pip install ipywebrtc notebook\n",
        "! apt install ffmpeg\n",
        "! apt-get install libportaudio2\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "from IPython.display import Audio, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Select File**\n",
        "\n",
        "*1.1 Local File*\n",
        "\n",
        "*1.2 Online Recording*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u7zRrytJvbLL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF_eOkuYbrBj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **1.1 Upload Local File**\n",
        "#@markdown You can upload any audio and video file, then it will be transformed as wav automatically.\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "import os\n",
        "use_drive = False\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "new_name = \"my_recording.wav\"\n",
        "os.rename(file_name, new_name)\n",
        "audio = AudioSegment.from_file(new_name)\n",
        "audio.export(new_name, format=\"wav\")\n",
        "\n",
        "!ls\n",
        "\n",
        "print('File uploadedï¼Œplease continue to upload more or execute next cell')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **1.2 Online Recording (Use your microphone)**\n",
        "#@markdown We need to enable some Colab widgets in order to make an audio recording.\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "WmCVlVnMAGQX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Once you've pressed the circle button, talk. When done, press the circle button once more. The widget will start playing back what it captured.\n",
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "recorder"
      ],
      "metadata": {
        "id": "-fFdSBBAGjFk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown PyTorch cannot read the above-captured audio format. We transform our recording into a format that PyTorch can understand in this step.\n",
        "\n",
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav my_recording.wav -y -hide_banner -loglevel panic"
      ],
      "metadata": {
        "id": "EDDgAohMGrCR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the actual test scene, the effect of online recording is not that ideal. So, I recommend to use vocaroo (https://vocaroo.com/) to make an online audio recording and then upload it."
      ],
      "metadata": {
        "id": "M_GBDWpAdeQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Models and Text Export**\n",
        "\n",
        "*2.1 Select and Run Modle*\n",
        "\n",
        "*2.2 Text Export*"
      ],
      "metadata": {
        "id": "Q32kCUXp2tll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2.1 Select and Run Modle**\n",
        "\n",
        "#@markdown Whisper is capable of detecting the input language and performing transcriptions for many languages. However, to be on the safe side, we can explicitly tell Whisper which language to expect.\n",
        "\n",
        "language_options = whisper.tokenizer.TO_LANGUAGE_CODE\n",
        "language_list = list(language_options.keys())"
      ],
      "metadata": {
        "id": "Q9ytPwEnXlSi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_dropdown = widgets.Dropdown(options=language_list, value='english')\n",
        "output = widgets.Output()\n",
        "display(lang_dropdown)"
      ],
      "metadata": {
        "id": "dpLnKvlb-vLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_dropdown = widgets.Dropdown(options=['transcribe', 'translate'], value='transcribe')\n",
        "output = widgets.Output()\n",
        "display(task_dropdown)"
      ],
      "metadata": {
        "id": "ilyDW-ALMnke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "#@markdown ---\n",
        "Model = 'small.en' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "#@markdown In the actual test scene, I made a compromise between accuracy and transcription speed, choosing the largest possible model while ensuring that the transcription speed was not too slow. Finally, a small model is a good option.\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if lang_dropdown.value == \"english\":\n",
        "  model = whisper.load_model(Model)\n",
        "else:\n",
        "  model = whisper.load_model(\"tiny\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ],
      "metadata": {
        "id": "TMhrSq_GZ6kA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwOUHau-dkUt"
      },
      "outputs": [],
      "source": [
        "options = whisper.DecodingOptions(language=lang_dropdown.value, task=task_dropdown.value, without_timestamps=True)\n",
        "options"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **2.2 Text Export**\n",
        "#@markdown All that's left to do now is feed our audio into Whisper.\n",
        "\n",
        "# Load the entire audio file\n",
        "audio = whisper.load_audio(\"my_recording.wav\")\n",
        "\n",
        "# Transcribe the audio using the model\n",
        "result = whisper.transcribe(model, audio)\n",
        "\n",
        "# Get the transcribed text\n",
        "text = result[\"text\"]\n",
        "\n",
        "# Split the text into individual sentences\n",
        "sentences = text.split('.')\n",
        "\n",
        "# Remove empty sentences\n",
        "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "# Format the sentences with line breaks\n",
        "formatted_text = '\\n'.join(sentences)\n",
        "\n",
        "# Print the formatted text\n",
        "print(formatted_text)\n"
      ],
      "metadata": {
        "id": "dOd6e1n31Crl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Analysis and Summary**"
      ],
      "metadata": {
        "id": "-n5aO_gA_gUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.1 OpenAI**\n",
        "#@markdown Install and import OpenAI\n",
        "!pip install openai\n",
        "import openai"
      ],
      "metadata": {
        "id": "aMvG3VFWSREB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.2 OpenAI API Key**\n",
        "#@markdown Imput OpenAI Api Key and enter\n",
        "from getpass import getpass\n",
        "openai.api_key = getpass()"
      ],
      "metadata": {
        "id": "ekM-FPHZHN47",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.3 Splits the text**\n",
        "#@markdown Divide the text into 4 parts evenly. 4 is random, according to the length of the text, you can customize.\n",
        "\n",
        "allwords=formatted_text.split(\" \")\n",
        "import numpy as np\n",
        "parts=np.array_split(allwords,4)\n",
        "parts"
      ],
      "metadata": {
        "id": "Pp8xybOBfFLX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.4 Rearrange the first paragraph**\n",
        "#@markdown To begin, let's rearrange the given phrases into coherent sentences and paragraphs. We'll start with the first paragraph\n",
        "para0=' '.join(list(parts[0]))\n",
        "para0"
      ],
      "metadata": {
        "id": "Ey5siwqkh_wO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.5 Summarize the first paragraph**\n",
        "prompt0=f\"{para0}\\n\\ntl;dr:\"\n",
        "\n",
        "response0 = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt0,\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        ")\n",
        "response0"
      ],
      "metadata": {
        "id": "MKrrtjJzjAJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.6 Only \"text\" in \"choices\"**\n",
        "summary0=response0[\"choices\"][0][\"text\"]\n",
        "summary0"
      ],
      "metadata": {
        "id": "_WXxQ_AimrE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.7 Summary**\n",
        "#@markdown Then run the for loop, and get all paragraphs of the summary.\n",
        "allsummary=[]\n",
        "\n",
        "for part in parts:\n",
        "\n",
        "  para=' '.join(list(part))\n",
        "\n",
        "  prompt=f\"{para}\\n\\ntl;dr:\"\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1.0,\n",
        "    frequency_penalty=0.0,\n",
        "    presence_penalty=1\n",
        "    )\n",
        "\n",
        "  summary=response[\"choices\"][0][\"text\"]\n",
        "  allsummary.append(summary)\n",
        "\n",
        "  result=\" \".join(allsummary)"
      ],
      "metadata": {
        "id": "utjwpuL4nD6x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Get Summary\n",
        "import textwrap\n",
        "print(textwrap.fill(result,150))"
      ],
      "metadata": {
        "id": "2jT-z_UgqpNp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. GUI**\n"
      ],
      "metadata": {
        "id": "r_DWnJeCAcog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **4.1 Install the Web UI Toolkit**\n",
        "#@markdown We'll be using Gradio to provide the widgets we need to do audio recording.\n",
        "! pip install gradio -q\n",
        "! pip install gradio torch torchaudio\n"
      ],
      "metadata": {
        "id": "Ewy3BtLDB2RM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "u7fCtAvWaDs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **4.2 Web Interface**\n",
        "#@markdown After running this script, you should see some widgets below that you can use to upload local file or record live audio, and see the text transcript.\n",
        "\n",
        "#@markdown Unfortunately, I failed to make this web interface export the summary. However, as long as you run this Notebook step by step, you will get what you want.\n",
        "import gradio as gr\n",
        "import textwrap\n",
        "model = whisper.load_model(\"small\")\n",
        "def your_transcription_function(audio):\n",
        "  audio = whisper.load_audio(audio)\n",
        "  result = whisper.transcribe(model, audio)\n",
        "  text = result[\"text\"]\n",
        "  sentences = text.split('.')\n",
        "  sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "  formatted_text = '\\n'.join(sentences)\n",
        "  return formatted_text\n",
        "def analyze_and_summarize(audio):\n",
        "    result = your_transcription_function(audio)\n",
        "    wrapped_text = textwrap.fill(result, width=150)\n",
        "    return wrapped_text\n",
        "def update_output_text(audio,record):\n",
        "    if audio != '':\n",
        "      audio = audio\n",
        "    else:\n",
        "      audio = record\n",
        "    summary = analyze_and_summarize(audio)\n",
        "    return summary\n",
        "\n",
        "audio_input = gr.inputs.Audio(source=\"upload\", label=\"Upload Audio File\",type = 'filepath')\n",
        "recording_input = gr.inputs.Audio(source=\"microphone\", label=\"Record Audio\",type = 'filepath')\n",
        "\n",
        "interface = gr.Interface(fn=update_output_text, inputs=[audio_input, recording_input],\n",
        "                         outputs=\"text\")\n",
        "interface.launch(share=True,debug=True)"
      ],
      "metadata": {
        "id": "vvWo9LV0LWBm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}